---
title: "06 - Model Fitting and Recommendation Systems Comprehension Check"
author: "Alessandro Corradini - Harvard Data Science Professional"
output:
  html_document: default
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

<br/>

## **Ensembles**

For these exercises we are going to build several machine learning models for the mnist_27 dataset and then build an ensemble. Each of the exercises in this comprehension check builds on the last.

Use the training set to build a model with several of the models available from the caret package. We will test out all of the following models in this exercise:

```{r, include=TRUE}
models <- c("glm", "lda",  "naive_bayes",  "svmLinear", 
                "gamboost",  "gamLoess", "qda", 
                "knn", "kknn", "loclda", "gam",
                "rf", "ranger",  "wsrf", "Rborist", 
                "avNNet", "mlp", "monmlp",
                "adaboost", "gbm",
                "svmRadial", "svmRadialCost", "svmRadialSigma")
```

We have not explained many of these, but apply them anyway using train with all the default parameters. You will likely need to install some packages. Keep in mind that you will probably get some warnings. Also, it will probably take a while to train all of the models - be patient!

Run the following code to train the various models:

```{r, include=TRUE}
library(caret)
library(dslabs)
set.seed(1)
data("mnist_27")

fits <- lapply(models, function(model){ 
	print(model)
	train(y ~ ., method = model, data = mnist_27$train)
}) 
    
names(fits) <- models
```

### **Question 2**

Now that you have all the trained models in a list, use ```sapply``` or ```map``` to create a matrix of predictions for the test set. You should end up with a matrix with ```length(mnist_27$test$y)``` rows and ```length(models)```.

What are the dimensions of the matrix of predictions?

```{r, include=TRUE}
pred <- sapply(fits, function(object) 
	predict(object, newdata = mnist_27$test))
dim(pred)
```

- Number of rows: 200
- Number of columns: 23


### **Question 3**

Now compute accuracy for each model on the test set. Report the mean accuracy across all models.

```{r, include=TRUE}
acc <- colMeans(pred == mnist_27$test$y)
acc
mean(acc)
```

### **Question 4**

Next, build an ensemble prediction by majority vote and compute the accuracy of the ensemble.

```{r, include=TRUE}
votes <- rowMeans(pred == "7")
y_hat <- ifelse(votes > 0.5, "7", "2")
mean(y_hat == mnist_27$test$y)
``` 

What is the accuracy of the ensemble? 0.84


### **Question 5**

In Q3, we computed the accuracy of each method on the training set and noticed that the individual accuracies varied.

How many of the individual methods do better than the ensemble? 1

Which individual methods perform better than the ensemble?

```{r, include=TRUE}
ind <- acc > mean(y_hat == mnist_27$test$y)
sum(ind)
models[ind]
```

- glm
- lda
- naive_bayes
- svmLinear
- gamboost
- gamLoess
- qda
- knn
- kknn
- loclda [X]
- gam
- rf
- ranger
- wsrf
- Rborist
- avNNet
- mlp
- monmlp
- adaboost
- gbm
- svmRadial
- svmRadialCost
- svmRadialSigma


### **Question 6**

It is tempting to remove the methods that do not perform well and re-do the ensemble. The problem with this approach is that we are using the test data to make a decision. However, we could use the accuracy estimates obtained from cross validation with the training data. Obtain these estimates and save them in an object. Report the mean accuracy of the new estimates.

```{r, include=TRUE}
acc_hat <- sapply(fits, function(fit) min(fit$results$Accuracy))
mean(acc_hat)
```

What is the mean accuracy of the new estimates? 0.811891



### **Question 7**

Now let's only consider the methods with an estimated accuracy of greater than or equal to 0.8 when constructing the ensemble.

```{r, include=TRUE}
ind <- acc_hat >= 0.8
votes <- rowMeans(pred[,ind] == "7")
y_hat <- ifelse(votes>=0.5, 7, 2)
mean(y_hat == mnist_27$test$y)
```

What is the accuracy of the ensemble now? 0.845


## **Dimension Reduction**
### **Question 1**

We want to explore the tissue_gene_expression predictors by plotting them.

```{r, include=TRUE}
library(dplyr)
data("tissue_gene_expression")
dim(tissue_gene_expression$x)
```

We want to get an idea of which observations are close to each other, but, as you can see from the dimensions, the predictors are 500-dimensional, making plotting difficult. Plot the first two principal components with color representing tissue type.

Which tissue is in a cluster by itself?

```{r, include=TRUE}
pc <- prcomp(tissue_gene_expression$x)
data.frame(pc_1 = pc$x[,1], pc_2 = pc$x[,2], 
			tissue = tissue_gene_expression$y) %>%
	ggplot(aes(pc_1, pc_2, color = tissue)) +
	geom_point()
```

- cerebellum
- colon
- endometrium
- hippocampus
- kidney
- liver [X]
- placenta




### **Question 2**

The predictors for each observation are measured using the same device and experimental procedure. This introduces biases that can affect all the predictors from one observation. For each observation, compute the average across all predictors, and then plot this against the first PC with color representing tissue. Report the correlation.

```{r, include=TRUE}
avgs <- rowMeans(tissue_gene_expression$x)
data.frame(pc_1 = pc$x[,1], avg = avgs, 
			tissue = tissue_gene_expression$y) %>%
ggplot(aes(avgs, pc_1, color = tissue)) +
	geom_point()
cor(avgs, pc$x[,1])
```

What is the correlation? 0.5969088


### **Question 3**

We see an association with the first PC and the observation averages. Redo the PCA but only after removing the center. Part of the code is provided for you.

```{r, include=TRUE}
x <- with(tissue_gene_expression, sweep(x, 1, rowMeans(x)))
pc <- prcomp(x)
data.frame(pc_1 = pc$x[,1], pc_2 = pc$x[,2], 
			tissue = tissue_gene_expression$y) %>%
	ggplot(aes(pc_1, pc_2, color = tissue)) +
	geom_point()
```

Which line of code should be used to replace #BLANK in the code block above?

- ```x <- with(tissue_gene_expression, sweep(x, 1, mean(x)))```
- ```x <- sweep(x, 1, rowMeans(tissue_gene_expression$x))```
- ```x <- tissue_gene_expression$x - mean(tissue_gene_expression$x)```
- ```x <- with(tissue_gene_expression, sweep(x, 1, rowMeans(x)))``` [X]

### **Question 4**

For the first 10 PCs, make a boxplot showing the values for each tissue.

For the 7th PC, which two tissues have the greatest median difference?

```{r, include=TRUE}
for(i in 1:10){
	boxplot(pc$x[,i] ~ tissue_gene_expression$y, main = paste("PC", i))
}
```

- cerebellum
- colon
- endometrium
- hippocampus
- kidney
- liver [X]
- placenta [X]


### **Question 5**

Plot the percent variance explained by PC number. Hint: use the ```summary``` function.

```{r, include=TRUE}
plot(summary(pc)$importance[3,])
```

How many PCs are required to reach a cumulative percent variance explained greater than 50%? 3


## **Recommendation Systems**

The following exercises all work with the movielens data, which can be loaded using the following code:

```{r, include=TRUE}
library(dslabs)
data("movielens")
```

### **Question 1**

Compute the number of ratings for each movie and then plot it against the year the movie came out. Use the square root transformation on the counts.

What year has the highest median number of ratings? 1995

```{r, include=TRUE}
movielens %>% group_by(movieId) %>%
	summarize(n = n(), year = as.character(first(year))) %>%
	qplot(year, n, data = ., geom = "boxplot") +
	coord_trans(y = "sqrt") +
	theme(axis.text.x = element_text(angle = 90, hjust = 1))
```


### **Question 2**

We see that, on average, movies that came out after 1993 get more ratings. We also see that with newer movies, starting in 1993, the number of ratings decreases with year: the more recent a movie is, the less time users have had to rate it.

Among movies that came out in 1993 or later, what are the 25 movies with the most ratings per year, and what is the average rating of each of the top 25 movies?

```{r, include=TRUE}
movielens %>% 
	filter(year >= 1993) %>%
	group_by(movieId) %>%
	summarize(n = n(), years = 2018 - first(year),
				title = title[1],
				rating = mean(rating)) %>%
	mutate(rate = n/years) %>%
	top_n(25, rate) %>%
	arrange(desc(rate)) 
```

- What is the average rating for the movie The Shawshank Redemption? 4.49
- What is the average number of ratings per year for the movie Forrest Gump? 14.2



### **Question 3**

From the table constructed in Q2, we can see that the most frequently rated movies tend to have above average ratings. This is not surprising: more people watch popular movies. To confirm this, stratify the post-1993 movies by ratings per year and compute their average ratings. Make a plot of average rating versus ratings per year and show an estimate of the trend.

What type of trend do you observe?

```{r, include=TRUE}
movielens %>% 
	filter(year >= 1993) %>%
	group_by(movieId) %>%
	summarize(n = n(), years = 2017 - first(year),
				title = title[1],
				rating = mean(rating)) %>%
	mutate(rate = n/years) %>%
	ggplot(aes(rate, rating)) +
	geom_point() +
	geom_smooth()
```

- There is no relationship between how often a movie is rated and its average rating.
- Movies with very few and very many ratings have the highest average ratings.
- The more often a movie is rated, the higher its average rating. [X]
- The more often a movie is rated, the lower its average rating.


### **Question 4**

Suppose you are doing a predictive analysis in which you need to fill in the missing ratings with some value.

Given your observations in the exercise in Q3, which of the following strategies would be most appropriate?

- Fill in the missing values with the average rating across all movies.
- Fill in the missing values with 0.
- Fill in the missing values with a lower value than the average rating across all movies. [X]
- Fill in the value with a higher value than the average rating across all movies.
- None of the above. 


### **Question 5**

The ```movielens``` dataset also includes a time stamp. This variable represents the time and data in which the rating was provided. The units are seconds since January 1, 1970. Create a new column ```date``` with the date.

Which code correctly creates this new column?

```{r, include=TRUE}
library(lubridate)
movielens <- mutate(movielens, date = as_datetime(timestamp))
```

- ```movielens <- mutate(movielens, date = as.date(timestamp))```
- ```movielens <- mutate(movielens, date = as_datetime(timestamp))``` [X]
- ```movielens <- mutate(movielens, date = as.data(timestamp))```
- ```movielens <- mutate(movielens, date = timestamp)```


### **Question 6**

Compute the average rating for each week and plot this average against day. Hint: use the round_date function before you group_by.

What type of trend do you observe?

```{r, include=TRUE}
movielens %>% mutate(date = round_date(date, unit = "week")) %>%
	group_by(date) %>%
	summarize(rating = mean(rating)) %>%
	ggplot(aes(date, rating)) +
	geom_point() +
	geom_smooth()
```

- There is strong evidence of a time effect on average rating.
- There is some evidence of a time effect on average rating. [X]
- There is no evidence of a time effect on average rating.

### **Question 7**

Consider again the plot you generated in Q6.

If we define $\ d_{u,i}$ as the day for user's $\ u$ rating of movie $\ i$, which of the following models is most appropriate?

- $\ Y_{u,i} = \mu + b_i + b_u + d_{u,i} + \varepsilon_{u,i}$
- $\ Y_{u,i} = \mu + b_i + b_u + d_{u,i}\beta + \varepsilon_{u,i}$
- $\ Y_{u,i} = \mu + b_i + b_u + d_{u,i}\beta_i + \varepsilon_{u,i}$
- $\ Y_{u,i} = \mu + b_i + b_u + f(d_{u,i}) + \varepsilon_{u,i}$, with $\ f$ a smooth function of  $\ d_{u,i}$ [X]

### **Question 8**

The movielens data also has a genres column. This column includes every genre that applies to the movie. Some movies fall under several genres. Define a category as whatever combination appears in this column. Keep only categories with more than 1,000 ratings. Then compute the average and standard error for each category. Plot these as error bar plots.

```{r, include=TRUE}
movielens %>% group_by(genres) %>%
	summarize(n = n(), avg = mean(rating), se = sd(rating)/sqrt(n())) %>%
	filter(n >= 1000) %>% 
	mutate(genres = reorder(genres, avg)) %>%
	ggplot(aes(x = genres, y = avg, ymin = avg - 2*se, ymax = avg + 2*se)) + 
	geom_point() +
	geom_errorbar() + 
	theme(axis.text.x = element_text(angle = 90, hjust = 1))
```

Which genre has the lowest average rating? Comedy


### **Question 9**

The plot you generated in Q8 shows strong evidence of a genre effect. Consider this plot as you answer the following question.

If we define $\ g_{u,i}$ as the day for user's $\ u$ rating of movie $\ i$, which of the following models is most appropriate?

-  $\ Y_{u,i} = \mu + b_i + b_u + g_{u,i} + \varepsilon_{u,i}$
-  $\ Y_{u,i} = \mu + b_i + b_u + g_{u,i}\beta + \varepsilon_{u,i}$
-  $\ Y_{u,i} = \mu + b_i + b_u + \sum{k=1}^K x_{u,i} \beta_k + \varepsilon_{u,i}$, with x^k_{u,i}$ = 1 if $\ g_{u,i}$ is genre $\ k$ [X]
-  $\ Y_{u,i} = \mu + b_i + b_u + f(g_{u,i}) + \varepsilon_{u,i}$, with  $\ f$ a smooth function of  $\ g_{u,i}$


## **Regularization**

The exercises in Q1-Q8 work with a simulated dataset for 100 schools. This pre-exercise setup walks you through the code needed to simulate the dataset.

An education expert is advocating for smaller schools. The expert bases this recommendation on the fact that among the best performing schools, many are small schools. Let's simulate a dataset for 100 schools. First, let's simulate the number of students in each school, using the following code:

```{r, include=TRUE}
set.seed(1986)
n <- round(2^rnorm(1000, 8, 1))
```

Now let's assign a true quality for each school that is completely independent from size. This is the parameter we want to estimate in our analysis. The true quality can be assigned using the following code:

```{r, include=TRUE}
set.seed(1)
mu <- round(80 + 2*rt(1000, 5))
range(mu)
schools <- data.frame(id = paste("PS",1:100),
                      size = n,
                      quality = mu,
                      rank = rank(-mu))
```

We can see the top 10 schools using this code: 

```{r, include=TRUE}
schools %>% top_n(10, quality) %>% arrange(desc(quality))
```

Now let's have the students in the school take a test. There is random variability in test taking, so we will simulate the test scores as normally distributed with the average determined by the school quality with a standard deviation of 30 percentage points. This code will simulate the test scores:

```{r, include=TRUE}
set.seed(1)
scores <- sapply(1:nrow(schools), function(i){
       scores <- rnorm(schools$size[i], schools$quality[i], 30)
       scores
})
schools <- schools %>% mutate(score = sapply(scores, mean))
```

### **Question 1**

What are the top schools based on the average score? Show just the ID, size, and the average score.

Report the ID of the top school and average score of the 10th school.

```{r, include=TRUE}
schools %>% top_n(10, score) %>% arrange(desc(score)) %>% select(id, size, score)
```

- What is the ID of the top school? 67
- What is the average score of the 10th school? 88.09490

 
### **Question 2**

Compare the median school size to the median school size of the top 10 schools based on the score.

```{r, include=TRUE}
median(schools$size)
schools %>% top_n(10, score) %>% .$size %>% median()
``` 

- What is the median school size overall? 261
- What is the median school size of the of the top 10 schools based on the score? 136



### **Question 3**

According to this analysis, it appears that small schools produce better test scores than large schools. Four out of the top 10 schools have 100 or fewer students. But how can this be? We constructed the simulation so that quality and size were independent. Repeat the exercise for the worst 10 schools.

```{r, include=TRUE}
median(schools$size)
schools %>% top_n(-10, score) %>% .$size %>% median()
```
What is the median school size of the bottom 10 schools based on the score? 146


### **Question 4**

From this analysis, we see that the worst schools are also small. Plot the average score versus school size to see what's going on. Highlight the top 10 schools based on the true quality. Use a log scale to transform for the size.

```{r, include=TRUE}
schools %>% ggplot(aes(size, score)) +
	geom_point(alpha = 0.5) +
	geom_point(data = filter(schools, rank<=10), col = 2) 
```

What do you observe?

- There is no difference in the standard error of the score based on school size; there must be an error in how we generated our data.
- The standard error of the score has larger variability when the school is smaller, which is why both the best and the worst schools are more likely to be small. [X]
- The standard error of the score has smaller variability when the school is smaller, which is why both the best and the worst schools are more likely to be small.
- The standard error of the score has larger variability when the school is very small or very large, which is why both the best and the worst schools are more likely to be small.
- The standard error of the score has smaller variability when the school is very small or very large, which is why both the best and the worst schools are more likely to be small.

### **Question 5**

Let's use regularization to pick the best schools. Remember regularization shrinks deviations from the average towards 0. To apply regularization here, we first need to define the overall average for all schools, using the following code:

```{r, include=TRUE}
overall <- mean(sapply(scores, mean))
```

Then, we need to define, for each school, how it deviates from that average.

Write code that estimates the score above the average for each school but dividing by $\ n + \alpha$ instead of $\ n$, with $\ n$ the schools size and $\ \alpha$ a regularization parameters. Try $\ \alpha = 25$.

```{r, include=TRUE}
alpha <- 25
score_reg <- sapply(scores, function(x)  overall + sum(x-overall)/(length(x)+alpha))
schools %>% mutate(score_reg = score_reg) %>%
	top_n(10, score_reg) %>% arrange(desc(score_reg))
```

What is the ID of the top school with regularization? 91
What is the regularized score of the 10th school? 86.90070


### **Question 6**

Notice that this improves things a bit. The number of small schools that are not highly ranked is now lower. Is there a better $\ \alpha$? Find the $\ \alpha$ that minimizes the RMSE = $\ \frac{1}{100} \sum_{i=1}^{100} (\mbox{quality} - \mbox{estimate})^2$.

```{r, include=TRUE}
alphas <- seq(10,250)
rmse <- sapply(alphas, function(alpha){
	score_reg <- sapply(scores, function(x) overall+sum(x-overall)/(length(x)+alpha))
	mean((score_reg - schools$quality)^2)
})
plot(alphas, rmse)
alphas[which.min(rmse)]  
```

What value of  gives the minimum RMSE? 128


### **Question 7**

Rank the schools based on the average obtained with the best $\ \alpha$. Note that no small school is incorrectly included.

```{r, include=TRUE}
alpha <- alphas[which.min(rmse)]  
score_reg <- sapply(scores, function(x)
	overall+sum(x-overall)/(length(x)+alpha))
schools %>% mutate(score_reg = score_reg) %>%
	top_n(10, score_reg) %>% arrange(desc(score_reg))
```

- What is the ID of the top school now? 91
- What is the regularized average score of the 10th school now? 85.35335

 
### **Question 8**

A common mistake made when using regularization is shrinking values towards 0 that are not centered around 0. For example, if we don't subtract the overall average before shrinking, we actually obtain a very similar result. Confirm this by re-running the code from the exercise in Q6 but without removing the overall mean.

```{r, include=TRUE}
alphas <- seq(10,250)
rmse <- sapply(alphas, function(alpha){
	score_reg <- sapply(scores, function(x) sum(x)/(length(x)+alpha))
	mean((score_reg - schools$quality)^2)
})
plot(alphas, rmse)
alphas[which.min(rmse)] 
```

What value of $\ \alpha$ gives the minimum RMSE here? 10


## **Matrix Factorization**

In this exercise set, we will be covering a topic useful for understanding matrix factorization: the singular value decomposition (SVD). SVD is a mathematical result that is widely used in machine learning, both in practice and to understand the mathematical properties of some algorithms. This is a rather advanced topic and to complete this exercise set you will have to be familiar with linear algebra concepts such as matrix multiplication, orthogonal matrices, and diagonal matrices.

The SVD tells us that we can decompose an $\ N\times p$ matrix $\ Y$ with $\ p < N$ as $\ Y = U D V^{\top}$

with $\ U$ and $\ V$ orthogonal of dimensions $\ N\times p$ and $\ p\times p$ respectively and$\  D a$\  p\times p$ diagonal matrix with the values of the diagonal decreasing: 
$\ d_{1,1} \geq d_{2,2} \geq \dots d_{p,p}$

In this exercise, we will see one of the ways that this decomposition can be useful. To do this, we will construct a dataset that represents grade scores for 100 students in 24 different subjects. The overall average has been removed so this data represents the percentage point each student received above or below the average test score. So a 0 represents an average grade (C), a 25 is a high grade (A+), and a -25 represents a low grade (F). You can simulate the data like this:

```{r, include=TRUE}
set.seed(1987)
n <- 100
k <- 8
Sigma <- 64  * matrix(c(1, .75, .5, .75, 1, .5, .5, .5, 1), 3, 3) 
m <- MASS::mvrnorm(n, rep(0, 3), Sigma)
m <- m[order(rowMeans(m), decreasing = TRUE),]
y <- m %x% matrix(rep(1, k), nrow = 1) + matrix(rnorm(matrix(n*k*3)), n, k*3)
colnames(y) <- c(paste(rep("Math",k), 1:k, sep="_"),
                 paste(rep("Science",k), 1:k, sep="_"),
                 paste(rep("Arts",k), 1:k, sep="_"))
```

Our goal is to describe the student performances as succinctly as possible. For example, we want to know if these test results are all just a random independent numbers. Are all students just about as good? Does being good in one subject  imply you will be good in another? How does the SVD help with all this? We will go step by step to show that with just three relatively small pairs of vectors we can explain much of the variability in this 100 x 24 dataset. 

### **Question 1**

You can visualize the 24 test scores for the 100 students by plotting an image:

```{r, include=TRUE}
my_image <- function(x, zlim = range(x), ...){
	colors = rev(RColorBrewer::brewer.pal(9, "RdBu"))
	cols <- 1:ncol(x)
	rows <- 1:nrow(x)
	image(cols, rows, t(x[rev(rows),,drop=FALSE]), xaxt = "n", yaxt = "n",
			xlab="", ylab="",  col = colors, zlim = zlim, ...)
	abline(h=rows + 0.5, v = cols + 0.5)
	axis(side = 1, cols, colnames(x), las = 2)
}

my_image(y)
```

How would you describe the data based on this figure?

- The test scores are all independent of each other.
- The students that are good at math are not good at science.
- The students that are good at math are not good at arts.
- The students that test well are at the top of the image and there seem to be three groupings by subject. [X]
- The students that test well are at the bottom of the image and there seem to be three groupings by subject.

### **Question 2**

You can examine the correlation between the test scores directly like this:

```{r, include=TRUE}
my_image(cor(y), zlim = c(-1,1))
range(cor(y))
axis(side = 2, 1:ncol(y), rev(colnames(y)), las = 2)
```

Which of the following best describes what you see?

- The test scores are independent.
- Test scores in math and science are highly correlated but scores in arts are not.
- There is high correlation between tests in the same subject but no correlation across subjects.
- There is correlation among all tests, but higher if the tests are in science and math and even higher within each subject. [X]

### **Question 3**

Remember that orthogonality means that $\ U^{\top}U$ and $\ V^{\top}V$ are equal to the identity matrix. This implies that we can also rewrite the decomposition as

$\ Y V = U D \mbox{ or } U^{\top}Y = D V^{\top}$

We can think of $\ YV$ and $\ U^{\top}V as two transformations of $\ Y$ that preserve the total variability of $\ Y$ since $\ U$ and $\ V$ are orthogonal.

Use the function ```svd``  to compute the SVD of ```y```. This function will return $\ U, V$, and the diagonal entries of $\ D$.

```{r, include=TRUE}
s <- svd(y)
names(s)
```

You can check that the SVD works by typing:

```{r, include=TRUE}
y_svd <- s$u %*% diag(s$d) %*% t(s$v)
max(abs(y - y_svd))
```

Compute the sum of squares of the columns of $\ Y$ and store them in ```ss_y```. Then compute the sum of squares of columns of the transformed YV and store them in ```ss_yv```. Confirm that ```sum(ss_y)``` is equal to ```sum(ss_yv)```.

```{r, include=TRUE}
y_sq <- y*y 
ss_y <- colSums(y_sq)
sum(ss_y) 
```

```{r, include=TRUE}
y_svd_sq <- y_svd*y_svd 
ss_yv <- colSums(y_svd_sq)
sum(ss_yv) 
```

What is the value of ```sum(ss_y)``` (and also the value of ```sum(ss_yv)```)? 175435

 
### **Question 4**

We see that the total sum of squares is preserved. This is because $\ V$ is orthogonal. Now to start understanding how $\ YV$ is useful, plot ```ss_y``` against the column number and then do the same for ```ss_yv```.

What do you observe?

```{r, include=TRUE}
plot(ss_y) 
plot(ss_yv)
```

- ```ss_y``` and ```ss_y```v are decreasing and close to 0 for the 4th column and beyond.
- ```ss_yv``` is decreasing and close to 0 for the 4th column and beyond. [X]
- ```ss_y``` is decreasing and close to 0 for the 4th column and beyond.
- There is no discernible pattern to either ```ss_y``` or ```ss_yv```.


### **Question 5**

Note that we didn't have to compute ```ss_yv``` because we already have the answer. How? Remember that $\ YV = UD$ and because $\ U$ is orthogonal, we know that the sum of squares of the columns of $\ UD$ are the diagonal entries of $\ D$ squared. Confirm this by plotting the square root of ```ss_y```v versus the diagonal entries of $\ D$.

What else is equal to $\ YV$?

```{r, include=TRUE}
plot(sqrt(ss_yv), s$d)
abline(0,1)
```

- D
- U
- UD [X]
- VUD


### **Question 6**

So from the above we know that the sum of squares of the columns of $\ Y$ (the total sum of squares) adds up to the sum of ```s$d^2``` and that the transformation $\ YV$ gives us columns with sums of squares equal to ```s$d^2```. Now compute the percent of the total variability that is explained by just the first three columns of $\ YV$.

```{r, include=TRUE}
sum(s$d[1:3]^2) / sum(s$d^2)
```

What proportion of the total variability is explained by the first three columns of $\ YV$? 0.988

 
### **Question 7**

Before we continue, let's show a useful computational trick to avoid creating the matrix ```diag(s$d)```. To motivate this, we note that if we write out in its columns $\ [U_1, U_2, \dots, U_p]$ then $\ UD$ is equal to $\ UD = [ U_1 d_{1,1}, U_2 d_{2,2}, \dots, U_p d_{p,p}]$

Use the ```sweep``` function to compute $\ UD$ without constructing ```diag(s$d)``` or using matrix multiplication.

Which code is correct?

```{r, include=TRUE}
identical(s$u %*% diag(s$d), sweep(s$u, 2, s$d, FUN = "*"))
```

- ```identical(t(s$u %*% diag(s$d)), sweep(s$u, 2, s$d, FUN = "*"))```
- ```identical(s$u %*% diag(s$d), sweep(s$u, 2, s$d, FUN = "*"))``` [X]
- ```identical(s$u %*% t(diag(s$d)), sweep(s$u, 2, s$d, FUN = "*"))```
- ```identical(s$u %*% diag(s$d), sweep(s$u, 2, s, FUN = "*"))```

### **Question 8**

We know that $\ U_1 d_{1,1}$, the first column of $\ UD$, has the most variability of all the columns of $\ UD$. Earlier we looked at an image of $\ Y$ using ```my_image(y)```, in which we saw that the student to student variability is quite large and that students that are good in one subject tend to be good in all. This implies that the average (across all subjects) for each student should explain a lot of the variability. Compute the average score for each student, plot it against $\ U_1 d_{1,1}$, and describe what you find.

What do you observe?

```{r, include=TRUE}
plot(-s$u[,1]*s$d[1], rowMeans(y))
```

- There is no relationship between the average score for each student and .
- There is a linearly decreasing relationship between the average score for each student and $\ U_1 d_{1,1}$.
- There is a linearly increasing relationship between the average score for each student and $\ U_1 d_{1,1}$. [X]
- There is an exponentially increasing relationship between the average score for each student and $\ U_1 d_{1,1}$.
- There is an exponentially decreasing relationship between the average score for each student and $\ U_1 d_{1,1}$.
Explanation


### **Question 9**

We note that the signs in SVD are arbitrary because:

$\ U D V^{\top} = (-U) D (-V)^{\top}$

With this in mind we see that the first column of $\ UD$ is almost identical to the average score for each student except for the sign.

This implies that multiplying $\ Y$ by the first column of $\ V$ must be performing a similar operation to taking the average. Make an image plot of $\ V$ and describe the first column relative to others and how this relates to taking an average.

How does the first column relate to the others, and how does this relate to taking an average?

```{r, include=TRUE}
my_image(s$v)
```

- The first column is very variable, which implies that the first column of YV is the sum of the rows of Y multiplied by some non-constant function, and is thus not proportional to an average.
- The first column is very variable, which implies that the first column of YV is the sum of the rows of Y multiplied by some non-constant function, and is thus proportional to an average. 
The first column is very close to being a constant, which implies that the - first column of YV is the sum of the rows of Y multiplied by some constant, and is thus proportional to an average. [X]
- The first three columns are all very close to being a constant, which implies that these columns are the sum of the rows of Y multiplied by some constant, and are thus proportional to an average.


### **Question 10**

We already saw that we can rewrite $\ UD$ as $\ U_1 d_{1,1} + U_2 d_{2,2} + \dots + U_p d_{p,p}$

with $\ U_j$ the j-th column of $\ U$. This implies that we can rewrite the entire SVD as:

$\ Y = U_1 d_{1,1} V_1 ^{\top} + U_2 d_{2,2} V_2 ^{\top} + \dots + U_p d_{p,p} V_p ^{\top}$

with $\ V_j $the jth column of $\ V$. Plot $\ U_1$, then plot $\ V_1^{\top}$ using the same range for the y-axis limits, then make an image of $\ U_1 d_{1,1} V_1 ^{\top}$ and compare it to the image of $\ Y$. Hint: use the ```my_image``` function defined above. Use the ```drop=FALSE``` argument to assure the subsets of matrices are matrices.

```{r, include=TRUE}
plot(s$u[,1], ylim = c(-0.25, 0.25))
plot(s$v[,1], ylim = c(-0.25, 0.25))
with(s, my_image((u[, 1, drop=FALSE]*d[1]) %*% t(v[, 1, drop=FALSE])))
my_image(y)
```

### **Question 11**

We see that with just a vector of length 100, a scalar, and a vector of length 24, we can actually come close to reconstructing the a 100 x 24 matrix. This is our first matrix factorization:

$\ Y \approx d_{1,1} U_1 V_1^{\top}$

In the exercise in Q6, we saw how to calculate the percent of total variability explained. However, our approximation only explains the observation that good students tend to be good in all subjects. Another aspect of the original data that our approximation does not explain was the higher similarity we observed within subjects. We can see this by computing the difference between our approximation and original data and then computing the correlations. You can see this by running this code:

```{r, include=TRUE}
resid <- y - with(s,(u[, 1, drop=FALSE]*d[1]) %*% t(v[, 1, drop=FALSE]))
my_image(cor(resid), zlim = c(-1,1))
axis(side = 2, 1:ncol(y), rev(colnames(y)), las = 2)
```

Now that we have removed the overall student effect, the correlation plot reveals that we have not yet explained the within subject correlation nor the fact that math and science are closer to each other than to the arts. So let's explore the second column of the SVD.

Repeat the previous exercise (Q10) but for the second column: Plot $\ U_2$, then plot $\ V_2^{\top}$ using the same range for the y-axis limits, then make an image of $\ U_2 d_{2,2} V_2 ^{\top}$ and compare it to the image of ```resid`` .

```{r, include=TRUE}
plot(s$u[,2], ylim = c(-0.5, 0.5))
plot(s$v[,2], ylim = c(-0.5, 0.5))
with(s, my_image((u[, 2, drop=FALSE]*d[2]) %*% t(v[, 2, drop=FALSE])))
my_image(resid)
```

### **Question 12**

The second column clearly relates to a student's difference in ability in math/science versus the arts. We can see this most clearly from the plot of ```s$v[,2]```. Adding the matrix we obtain with these two columns will help with our approximation:

$\ Y \approx d_{1,1} U_1 V_1^{\top} + d_{2,2} U_2 V_2^{\top}$

We know it will explain ```sum(s$d[1:2]^2)/sum(s$d^2) * 100``` percent of the total variability. We can compute new residuals like this:

```{r, include=TRUE}
resid <- y - with(s,sweep(u[, 1:2], 2, d[1:2], FUN="*") %*% t(v[, 1:2]))
my_image(cor(resid), zlim = c(-1,1))
axis(side = 2, 1:ncol(y), rev(colnames(y)), las = 2)
```

and see that the structure that is left is driven by the differences between math and science. Confirm this by first plotting $\ U_3$, then plotting $\ V_3^{\top}$ using the same range for the y-axis limits, then making an image of $\ U_3 d_{3,3} V_3 ^{\top}$ and comparing it to the image of ```resid```.


```{r, include=TRUE}
plot(s$u[,3], ylim = c(-0.5, 0.5))
plot(s$v[,3], ylim = c(-0.5, 0.5))
with(s, my_image((u[, 3, drop=FALSE]*d[3]) %*% t(v[, 3, drop=FALSE])))
my_image(resid)
```

### **Question 13**

The third column clearly relates to a student's difference in ability in math and science. We can see this most clearly from the plot of ```s$v[,3]```. Adding the matrix we obtain with these two columns will help with our approximation:

$\ Y \approx d_{1,1} U_1 V_1^{\top} + d_{2,2} U_2 V_2^{\top} + d_{3,3} U_3 V_3^{\top}$

We know it will explain: sum(s$d[1:3]^2)/sum(s$d^2) * 100 percent of the total variability. We can compute new residuals like this:

```{r, include=TRUE}
resid <- y - with(s,sweep(u[, 1:3], 2, d[1:3], FUN="*") %*% t(v[, 1:3]))
my_image(cor(resid), zlim = c(-1,1))
axis(side = 2, 1:ncol(y), rev(colnames(y)), las = 2)
```

We no longer see structure in the residuals: they seem to be independent of each other. This implies that we can describe the data with the following model:

$\ Y =  d_{1,1} U_1 V_1^{\top} + d_{2,2} U_2 V_2^{\top} + d_{3,3} U_3 V_3^{\top} + \varepsilon$

with $\ \varepsilon$ a matrix of independent identically distributed errors. This model is useful because we summarize of 100 x 24 observations with 3 X (100+24+1) = 375 numbers.

Furthermore, the three components of the model have useful interpretations:

1 - the overall ability of a student
2 - the difference in ability between the math/sciences and arts
3 - the remaining differences between the three subjects.

The sizes $\ d_{1,1}, d_{2,2}$ and $\ d_{3,3$} tell us the variability explained by each component. Finally, note that the components $\ d_{j,j} U_j V_j^{\top}$ are equivalent to the jth principal component.

Finish the exercise by plotting an image of $\ Y$, an image of $\ d_{1,1} U_1 V_1^{\top} + d_{2,2} U_2 V_2^{\top} + d_{3,3} U_3 V_3^{\top}$ and an image of the residuals, all with the same ```zlim```.

```{r, include=TRUE}
y_hat <- with(s,sweep(u[, 1:3], 2, d[1:3], FUN="*") %*% t(v[, 1:3]))
my_image(y, zlim = range(y))
my_image(y_hat, zlim = range(y))
my_image(y - y_hat, zlim = range(y))
```

## **Clustering**

These exercises will work with the tissue_gene_expression dataset, which is part of the dslabs package.

### **Question 1**

Load the ```tissue_gene_expression``` dataset. Remove the row means and compute the distance between each observation. Store the result in ```d```.

Which of the following lines of code correctly does this computation?

```{r, include=TRUE}
d <- dist(tissue_gene_expression$x - rowMeans(tissue_gene_expression$x))
```

- ```d <- dist(tissue_gene_expression$x)```
- ```d <- dist(rowMeans(tissue_gene_expression$x))```
- ```d <- dist(rowMeans(tissue_gene_expression$y))```
- ```d <- dist(tissue_gene_expression$x - rowMeans(tissue_gene_expression$x))```[X]

### **Question 2**

Make a hierarchical clustering plot and add the tissue types as labels.

You will observe multiple branches.

Which tissue type is in the branch farthest to the left?

```{r, include=TRUE}
h <- hclust(d)
plot(h)
```
- cerebellum
- colon
- endometrium
- hippocampus
- kidney
- liver [X]
- placenta


### **Question 3**

Run a k-means clustering on the data with $\ K = 7$. Make a table comparing the identified clusters to the actual tissue types. Run the algorithm several times to see how the answer changes.

What do you observe for the clustering of the liver tissue?

```{r, include=TRUE}
cl <- kmeans(tissue_gene_expression$x, centers = 7)
table(cl$cluster, tissue_gene_expression$y)
```

- Liver is always classified in a single cluster.
- Liver is never classified in a single cluster.
- Liver is classified in a single cluster roughly 20% of the time and in more than one cluster roughly 80% of the time. [X]
- Liver is classified in a single cluster roughly 80% of the time and in more than one cluster roughly 20% of the time.


### **Question 4**

Select the 50 most variable genes. Make sure the observations show up in the columns, that the predictor are centered, and add a color bar to show the different tissue types. Hint: use the ColSideColors argument to assign colors. Also, use col = RColorBrewer::brewer.pal(11, "RdBu") for a better use of colors.

Part of the code is provided for you here:

```{r, include=TRUE}
library(RColorBrewer)
sds <- matrixStats::colSds(tissue_gene_expression$x)
ind <- order(sds, decreasing = TRUE)[1:50]
colors <- brewer.pal(7, "Dark2")[as.numeric(tissue_gene_expression$y)]
heatmap(t(tissue_gene_expression$x[,ind]), col = brewer.pal(11, "RdBu"), scale = "row", ColSideColors = colors)
```

Which line of code should replace #BLANK in the code above?

- ```heatmap(t(tissue_gene_expression$x[,ind]), col = brewer.pal(11, "RdBu"), scale = "row", ColSideColors = colors)``` [X]
- ```heatmap(t(tissue_gene_expression$x[,ind]), col = brewer.pal(11, "RdBu"), scale = "row", ColSideColors = rev(colors))```
- ```heatmap(t(tissue_gene_expression$x[,ind]), col = brewer.pal(11, "RdBu"), scale = "row", ColSideColors = sample(colors))```
- ```heatmap(t(tissue_gene_expression$x[,ind]), col = brewer.pal(11, "RdBu"), scale = "row", ColSideColors = sample(colors))```
