---
title: "04 - Distance, Knn, Cross Validation, and Generative Models Comprehension Check"
author: "Alessandro Corradini - Harvard Data Science Professional"
output:
  html_document: default
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

<br/>

## **Distance**
### **Question 1**

Load the following dataset:

```{r, include=TRUE}
library(dslabs)
data("tissue_gene_expression")
```

This dataset includes a matrix ```x```:

```{r, include=TRUE}
dim(tissue_gene_expression$x)
```

This matrix has the gene expression levels of 500 genes from 189 biological samples representing seven different tissues. The tissue type is stored in ```y```:

```{r, include=TRUE}
table(tissue_gene_expression$y)
```

Which of the following lines of code computes the Euclidean distance between each observation and stores it in the object ```d```?

```{r, include=TRUE}
d <- dist(tissue_gene_expression$x)
```

- ```d <- dist(tissue_gene_expression$x, distance='maximum')```
- ```d <- dist(tissue_gene_expression)```
- ```d <- dist(tissue_gene_expression$x)``` [X]
- ```d <- cor(tissue_gene_expression$x)```

<br/>

### **Question 2**

Compare the distances between observations 1 and 2 (both cerebellum), observations 39 and 40 (both colon), and observations 73 and 74 (both endometrium).

Distance-wise, are samples from tissues of the same type closer to each other?

```{r, include=TRUE}
ind <- c(1, 2, 39, 40, 73, 74)
as.matrix(d)[ind,ind]
```

- No, the samples from the same tissue type are not necessarily closer.
- The two colon samples are closest to each other, but the samples from the other two tissues are not.
- The two cerebellum samples are closest to each other, but the samples from the other two tissues are not.
- Yes, the samples from the same tissue type are closest to each other. [X]


### **Question 3**

Make a plot of all the distances using the ```image``` function to see if the pattern you observed in Q2 is general.

Which code would correctly make the desired plot?

- ```image(d)```
- ```image(as.matrix(d))``` [X]
- ```d```
- ```image()```

## **Nearest Neighbors**
### **Question 1**

Previously, we used logistic regression to predict sex based on height. Now we are going to use knn to do the same. Use the code described in these videos to select the 
```F_1``` measure and plot it against ```k```. Compare to the ```F_1``` of about 0.6 we obtained with regression. Set the seed to 1.

- What is the max value of ```F_1```? 0.6019417
- At what value of ```k``` does the max occur? 40

```{r, include=TRUE}
set.seed(1)
data("heights")
library(caret)
library(dplyr)
ks <- seq(1, 101, 3)
F_1 <- sapply(ks, function(k){
	test_index <- createDataPartition(heights$sex, times = 1, p = 0.5, list = FALSE)
	test_set <- heights[test_index, ]
	train_set <- heights[-test_index, ]
	fit <- knn3(sex ~ height, data = train_set, k = k)
	y_hat <- predict(fit, test_set, type = "class") %>% 
		factor(levels = levels(train_set$sex))
	F_meas(data = y_hat, reference = test_set$sex)
})
plot(ks, F_1)
max(F_1)
```

### **Question 2**

Next we will use the same gene expression example used in the Comprehension Check: Distance exercises. You can load it like this:

```{r, include=TRUE}
library(dslabs)
data("tissue_gene_expression")
```

Split the data into training and test sets, and report the accuracy you obtain. Try it for ```k = 1, 3, 5, 7, 9, 11```. Set the seed to 1.

```{r, include=TRUE}
set.seed(1)
library(caret)
y <- tissue_gene_expression$y
x <- tissue_gene_expression$x
train_index <- createDataPartition(y, list = FALSE)
sapply(seq(1, 11, 2), function(k){
	fit <- knn3(x[train_index,], y[train_index], k = k)
	y_hat <- predict(fit, newdata = data.frame(x=x[-train_index,]),
				type = "class")
mean(y_hat == y[-train_index])
})
```

- k=1: 0.980
- k=3: 0.969
- k=5: 0.989
- k=7: 0.968
- k=9: 0.957
- k=11: 0.957

## **Cross-validation**
### **Question 1**

Generate a set of random predictors and outcomes using the following code:

```{r, include=TRUE}
library(dbplyr)
library(dslabs)
library(tidyverse)
library(caret)

set.seed(1996)
n <- 1000
p <- 10000
x <- matrix(rnorm(n*p), n, p)
colnames(x) <- paste("x", 1:ncol(x), sep = "_")
y <- rbinom(n, 1, 0.5) %>% factor()

x_subset <- x[ ,sample(p, 100)]
```

Because ```x``` and ```y``` are completely independent, you should not be able to predict ```y``` using ```x``` with accuracy greater than 0.5. Confirm this by running cross-validation using logistic regression to fit the model. Because we have so many predictors, we selected a random sample ```x_subset```. Use the subset when training the model.

Which code correctly performs this cross-validation?

```{r, include=TRUE}
fit <- train(x_subset, y, method = "glm")
fit$results
```

```
fit <- train(x_subset, y)
fit$results
```
```
fit <- train(x_subset, y, method = "glm")
fit$results [X]
```
```
fit <- train(y, x_subset, method = "glm")
fit$results
```
```
fit <- test(x_subset, y, method = "glm")
fit$results
```

### **Question 2**

Now, instead of using a random selection of predictors, we are going to search for those that are most predictive of the outcome. We can do this by comparing the values for $\ y = 1$ the  group to those in the $\ y = 0$  group, for each predictor, using a t-test. You can do perform this step like this:

```{r, include=TRUE}
library(devtools)
devtools::install_bioc("genefilter")
library(genefilter)
tt <- colttests(x, y)
```

Which of the following lines of code correctly creates a vector of the p-values called ```pvals```?

- ```pvals <- tt$dm```
- ```pvals <- tt$statistic```
- ```pvals <- tt```
- ```pvals <- tt$p.value``` [X]


```{r, include=TRUE}
pvals <- tt$p.value
```

### **Question 3**

Create an index ```ind``` with the column numbers of the predictors that were "statistically significantly" associated with ```y```. Use a p-value cutoff of 0.01 to define "statistically significantly."

```{r, include=TRUE}
ind <- which(pvals <= 0.01)
length(ind)
```

How many predictors survive this cutoff? 108

### **Question 4**

Now re-run the cross-validation after redefinining ```x_subset``` to be the subset of ```x``` defined by the columns showing "statistically significant" association with ```y```.

```{r, include=TRUE}
x_subset <- x[,ind]
fit <- train(x_subset, y, method = "glm")
fit$results
```

What is the accuracy now? 0.7571395



### **Question 5**

Re-run the cross-validation again, but this time using kNN. Try out the following grid ```k = seq(101, 301, 25)``` of tuning parameters. Make a plot of the resulting accuracies.

Which code is correct?

```
fit <- train(x_subset, y, method = "knn", tuneGrid = data.frame(k = seq(101, 301, 25)))
ggplot(fit) [X]
```
```
fit <- train(x_subset, y, method = "knn")
ggplot(fit)
```
```
fit <- train(x_subset, y, method = "knn", tuneGrid = data.frame(k = seq(103, 301, 25)))
ggplot(fit)
```
```
fit <- train(x_subset, y, method = "knn", tuneGrid = data.frame(k = seq(101, 301, 5)))
ggplot(fit)
```

### **Question 6**

In the previous exercises, we see that despite the fact that ```x``` and ```y``` are completely independent, we were able to predict ```y``` with accuracy higher than 70%. We must be doing something wrong then.

What is it?

- The function train estimates accuracy on the same data it uses to train the algorithm.
- We are overfitting the model by including 100 predictors.
- - We used the entire dataset to select the columns used in the model. [X]
The high accuracy is just due to random variability.


### **Question 7**

Use the ```train``` function to predict tissue from gene expression in the ```tissue_gene_expression``` dataset. Use kNN.

```{r, include=TRUE}
data("tissue_gene_expression")
fit <- with(tissue_gene_expression, train(x, y, method = "knn", tuneGrid = data.frame( k = seq(1, 7, 2))))
ggplot(fit)
fit$results
```

What value of ```k``` works best? 1

## **Bootstrap**
### **Question 1**

The ```createResample``` function can be used to create bootstrap samples. For example, we can create 10 bootstrap samples for the ```mnist_27``` dataset like this:


set.seed(1995)
indexes <- createResample(mnist_27$train$y, 10)


How many times do 3, 4, and 7 appear in the first resampled index?

```{r,include=TRUE}
sum(indexes[[1]] == 3)
sum(indexes[[1]] == 4)
sum(indexes[[1]] == 7)
```

- Enter the number of times 3 appears: 1
- Enter the number of times 4 appears: 4
- Enter the number of times 7 appears: 0


### **Question 2**

We see that some numbers appear more than once and others appear no times. This has to be this way for each dataset to be independent. Repeat the exercise for all the resampled indexes.

```{r,include=TRUE}
x=sapply(indexes, function(ind){
	sum(ind == 3)
})
sum(x)
```

What is the total number of times that 3 appears in all of the resampled indexes? 11


### **Question 3**

Generate a random dataset using the following code:

```{r,include=TRUE}
set.seed(1)
y <- rnorm(100, 0, 1)
```

Estimate the 75th quantile, which we know is ```qnorm(0.75)```, with the sample quantile: ```quantile(y, 0.75)```.

Run a Monte Carlo simulation with 10,000 repetitions to learn the expected value and standard error of this random variable. Set the seed to 1.

```{r,include=TRUE}
set.seed(1)
B <- 10000
q_75 <- replicate(B, {
	y <- rnorm(100, 0, 1)
	quantile(y, 0.75)
})

mean(q_75)
sd(q_75)
```

- Expected value 0.6655976
- Standard error 0.1353847

 
### **Question 4**

In practice, we can't run a Monte Carlo simulation. Use 10 bootstrap samples to estimate the standard error using just the initial sample ```y``` . Set the seed to 1.

```{r,include=TRUE}
set.seed(1)
indexes <- createResample(y, 10)
q_75_star <- sapply(indexes, function(ind){
	y_star <- y[ind]
	quantile(y_star, 0.75)
})
mean(q_75_star)
sd(q_75_star)
```

- Expected value 0.7312648
- Standard error 0.07419278

 
### **Question 5**

Repeat the exercise from Q4 but with 10,000 bootstrap samples instead of 10. Set the seed to 1.

```{r,include=TRUE}
set.seed(1)
indexes <- createResample(y, 10000)
q_75_star <- sapply(indexes, function(ind){
	y_star <- y[ind]
	quantile(y_star, 0.75)
})
mean(q_75_star)
sd(q_75_star)
```

- Expected value 0.6737512
- Standard error 0.0930575


### **Question 6**

Compare the SD values obtained using 10 vs 10,000 bootstrap samples.

What do you observe?

- The SD is substantially lower with 10,000 bootstrap samples than with 10.
- The SD is roughly the same in both cases. [X]
- The SD is substantially higher with 10,000 bootstrap samples than with 10. 

## **Generative Models**

In the following exercises, we are going to apply LDA and QDA to the ```tissue_gene_expression``` dataset. We will start with simple examples based on this dataset and then develop a realistic example.

### **Question 1**

Create a dataset of samples from just cerebellum and hippocampus, two parts of the brain, and a predictor matrix with 10 randomly selected columns using the following code:

```{r, include=TRUE}
set.seed(1993)
data("tissue_gene_expression")
ind <- which(tissue_gene_expression$y %in% c("cerebellum", "hippocampus"))
y <- droplevels(tissue_gene_expression$y[ind])
x <- tissue_gene_expression$x[ind, ]
x <- x[, sample(ncol(x), 10)]
```

Use the train function to estimate the accuracy of LDA.

```{r, include=TRUE}
fit_lda <- train(x, y, method = "lda")
fit_lda$results["Accuracy"]
```

What is the accuracy? 0.8707879




### **Question 2**

In this case, LDA fits two 10-dimensional normal distributions. Look at the fitted model by looking at the ```finalModel``` component of the result of ```train```. Notice there is a component called ```means``` that includes the estimated means of both distributions. Plot the mean vectors against each other and determine which predictors (genes) appear to be driving the algorithm.

Which TWO genes appear to be driving the algorithm?

```{r, include=TRUE}
t(fit_lda$finalModel$means) %>% data.frame() %>%
	mutate(predictor_name = rownames(.)) %>%
	ggplot(aes(cerebellum, hippocampus, label = predictor_name)) +
	geom_point() +
	geom_text() +
	geom_abline()
```

- PLCB1
- RAB1B [X]
- MSH4
- OAZ2 [X]
- SPI1
- SAPCD1
- HEMK1


### **Question 3**

Repeat the exercise in Q1 with QDA.

Create a dataset of samples from just cerebellum and hippocampus, two parts of the brain, and a predictor matrix with 10 randomly selected columns using the following code:

```{r, include=TRUE}
set.seed(1993)
data("tissue_gene_expression")
ind <- which(tissue_gene_expression$y %in% c("cerebellum", "hippocampus"))
y <- droplevels(tissue_gene_expression$y[ind])
x <- tissue_gene_expression$x[ind, ]
x <- x[, sample(ncol(x), 10)]
```

Use the train function to estimate the accuracy of QDA.

```{r, include=TRUE}
fit_qda <- train(x, y, method = "qda")
fit_qda$results["Accuracy"]
```

What is the accuracy? 0.8147954

### **Question 4**

Which TWO genes drive the algorithm when using QDA instead of LDA?

```{r, include=TRUE}
t(fit_qda$finalModel$means) %>% data.frame() %>%
	mutate(predictor_name = rownames(.)) %>%
	ggplot(aes(cerebellum, hippocampus, label = predictor_name)) +
	geom_point() +
	geom_text() +
	geom_abline()
```

- PLCB1
- RAB1B [X]
- MSH4
- OAZ2 [X]
- SPI1
- SAPCD1
- HEMK1

### **Question 5**

One thing we saw in the previous plots is that the values of the predictors correlate in both groups: some predictors are low in both groups and others high in both groups. The mean value of each predictor found in ```colMeans(x)``` is not informative or useful for prediction and often for purposes of interpretation, it is useful to center or scale each column. This can be achieved with the ```preProcessing``` argument in train. Re-run LDA with ```preProcessing = "scale"```. Note that accuracy does not change, but it is now easier to identify the predictors that differ more between groups than based on the plot made in Q2.

Which TWO genes drive the algorithm after performing the scaling?

```{r, include=TRUE}
fit_lda <- train(x, y, method = "lda", preProcess = "center")
fit_lda$results["Accuracy"]
t(fit_lda$finalModel$means) %>% data.frame() %>%
	mutate(predictor_name = rownames(.)) %>%
	ggplot(aes(predictor_name, hippocampus)) +
	geom_point() +
	coord_flip()
	
d <- apply(fit_lda$finalModel$means, 2, diff)
ind <- order(abs(d), decreasing = TRUE)[1:2]
plot(x[, ind], col = y)
```	

- C21orf62
- PLCB1
- RAB1B
- MSH4
- OAZ2 [X]
- SPI1 [X]
- SAPCD1
- IL18R1


### **Question 6**

Now we are going to increase the complexity of the challenge slightly: we will consider all the tissue types. Use the following code to create your dataset:

```{r, include=TRUE}
set.seed(1993)
data("tissue_gene_expression")
y <- tissue_gene_expression$y
x <- tissue_gene_expression$x
x <- x[, sample(ncol(x), 10)]


fit_lda <- train(x, y, method = "lda", preProcess = c("center"))
fit_lda$results["Accuracy"]
```	

What is the accuracy using LDA? 0.8194837