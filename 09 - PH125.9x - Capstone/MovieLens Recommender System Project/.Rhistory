knitr::opts_chunk$set(echo = TRUE)
install.packages("swirl")
library(swirl)
## install.packages("swirl")
library(swirl)
version
## install.packages("swirl")
library(swirl)
class(cars)
library(downloader)
install.packages("downloader")
library(downloader)
url <- "https://raw.githubusercontent.com/genomicsclass/dagdata/master/inst/extdata/femaleMiceWeights.csv"
filename <- "femaleMiceWeights.csv"
download(url, destfile=filename)
dat <- read.csv("femaleMiceWeights.csv")
head(dat)
install.packages("rafalib")
knitr::opts_chunk$set(echo = TRUE)
library(downloader)
url <- "https://raw.githubusercontent.com/genomicsclass/dagdata/master/inst/extdata/femaleControlsPopulation.csv"
filename <- basename(url)
download(url, destfile=filename)
x <- unlist( read.csv(filename) )
library(rafalib)
mypar(1,2)
hist(averages5, xlim=c(18,30))
library(rafalib)
set.seed(1)
n <- 1000
averages5 <- vector("numeric",n)
for(i in 1:n){
X <- sample(x,5)
averages5[i] <- mean(X)
}
hist(averages5)
set.seed(1) #so that we get same results
n <- 1000
averages50 <- vector("numeric",n)
for(i in 1:n){
X <- sample(x,50)
averages50[i] <- mean(X)
}
hist(averages50) ##take a look
mypar(1,2)
hist(averages5, xlim=c(18,30))
hist(averages50, xlim=c(18,30))
install.packages("dplyr")
library(dplyr)
x <- filter(dat, Sex=="M" & Diet=="chow") %>% select(Bodyweight) %>% unlist
library(downloader)
url <- "https://raw.githubusercontent.com/genomicsclass/dagdata/master/inst/extdata/mice_pheno.csv"
filename <- basename(url)
download(url, destfile=filename)
dat <- read.csv(filename)
# We will remove the lines that contain missing values:
dat <- na.omit( dat )
library(dplyr)
x <- filter(dat, Sex=="M" & Diet=="chow") %>% select(Bodyweight) %>% unlist
mean(x)
knitr::opts_chunk$set(echo = TRUE)
library(downloader)
library(dplyr)
url <- "https://raw.githubusercontent.com/genomicsclass/dagdata/master/inst/extdata/babies.txt"
filename <- basename(url)
download(url, destfile=filename)
babies <- read.table("babies.txt", header=TRUE)
knitr::opts_chunk$set(echo = TRUE)
library(rafalib)
set.seed(1)
N <- 5
X <- rnorm(N)
sqrt(N)*mean(X)/sd(X)
set.seed(1)
N <- 5
B<- 1000
tstats <- replicate(B,{
X <- rnorm(N)
sqrt(N)*mean(X)/sd(X)
})
mean(tstats>2)
Ns<-seq(5,30,5)
B <- 1000
mypar(3,2)
LIM <- c(-4.5,4.5)
for(N in Ns){
ts <- replicate(B,{
x <- rnorm(N)
y <- rnorm(N)
t.test(x,y, var.equal = TRUE)$stat
})
ps <- seq(1/(B+1),1-1/(B+1),len=B)
qqplot(qt(ps,df=2*N-2),ts,main=N,
xlab="Theoretical",ylab="Observed",
xlim=LIM, ylim=LIM)
abline(0,1)
}
set.seed(1)
N <- 15
B <- 10000
tstats <- replicate(B,{
X <- sample(c(-1,1), N, replace=TRUE)
sqrt(N)*mean(X)/sd(X)
})
ps=seq(1/(B+1), 1-1/(B+1), len=B)
qqplot(qt(ps,N-1), tstats, xlim=range(tstats))
abline(0,1)
set.seed(1)
Ns <- seq(5,45,5)
library(rafalib)
mypar(3,3)
for(N in Ns){
medians <- replicate(10000, median ( rnorm(N) ) )
title <- paste("N=",N,", avg=",round( mean(medians), 2) , ", sd*sqrt(N)=", round( sd(medians)*sqrt(N),2) )
qqnorm(medians, main = title )
qqline(medians)
}
url <- "https://raw.githubusercontent.com/genomicsclass/dagdata/master/inst/extdata/babies.txt"
filename <- basename(url)
download(url, destfile=filename)
babies <- read.table("babies.txt", header=TRUE)
bwt.nonsmoke <- filter(babies, smoke==0) %>% select(bwt) %>% unlist
bwt.smoke <- filter(babies, smoke==1) %>% select(bwt) %>% unlist
N=10
set.seed(1)
nonsmokers <- sample(bwt.nonsmoke , N)
smokers <- sample(bwt.smoke , N)
obs <- mean(smokers) - mean(nonsmokers)
dat <- c(smokers,nonsmokers)
shuffle <- sample( dat )
smokersstar <- shuffle[1:N]
nonsmokersstar <- shuffle[(N+1):(2*N)]
mean(smokersstar)-mean(nonsmokersstar)
set.seed(1)
null <- replicate(1000, {
shuffle <- sample( dat )
smokersstar <- shuffle[1:N]
nonsmokersstar <- shuffle[(N+1):(2*N)]
mean(smokersstar)-mean(nonsmokersstar)
})
( sum( abs(null) >= abs(obs)) +1 ) / ( length(null)+1 )
##we add the 1s to avoid p-values=0 but we also accept:
( sum( abs(null) >= abs(obs)) ) / ( length(null) )
set.seed(1)
obs <- median(smokers) - median(nonsmokers)
null <- replicate(1000, {
shuffle <- sample( dat )
smokersstar <- shuffle[1:N]
nonsmokersstar <- shuffle[(N+1):(2*N)]
median(smokersstar)-median(nonsmokersstar)
})
( sum( abs(null) >= abs(obs)) +1 ) / ( length(null)+1 )
## As before we add 1 to avoid p-value of 0 but we also accept
( sum( abs(null) >= abs(obs)) ) / ( length(null) )
d = read.csv("assoctest.csv")
tab = table(d$allele, d$case)
chisq.test(tab)
tab = table(d$allele, d$case)
fisher.test(tab)
install.packages("UsingR")
knitr::opts_chunk$set(echo = TRUE)
library(dplyr)
library(rafalib)
data(nym.2002, package="UsingR")
knitr::opts_chunk$set(echo = TRUE)
install_bioc(genefilter)
knitr::opts_chunk$set(echo = TRUE)
Ns <- c(5,10,30,100)
ps <- c(0.01,0.10,0.5,0.9,0.99)
library(rafalib)
mypar(4,5)
for(N in Ns){
ks <- 1:(N-1)
for(p in ps){
exact = dbinom(ks,N,p)
a = (ks+0.5 - N*p)/sqrt(N*p*(1-p))
b = (ks-0.5 - N*p)/sqrt(N*p*(1-p))
approx = pnorm(a) - pnorm(b)
LIM <- range(c(approx,exact))
plot(exact,approx,main=paste("N =",N," p = ",p),xlim=LIM,ylim=LIM,col=1,pch=16)
abline(0,1)
}
}
knitr::opts_chunk$set(echo = TRUE)
length(d)
install.packages("gplots")
install.packages("caret")
knitr::opts_chunk$set(echo = TRUE)
install_bioc("sva")
library(sva)
library(Biobase)
library(GSE5859Subset)
data(GSE5859Subset)
install.packages("digest")
install.packages(c("Rcpp", "digest"))
install.packages("Rcpp", repo = "https://cloud.r-project.org/" )
install.packages("Rcpp", repo = "https://cloud.r-project.org/")
install.packages("stringi")
install.packages("stringi", repos = "https://cloud.r-project.org/")
install.packages("devtools")
install.packages("rmarkdown")
install.packages("backports")
knitr::opts_chunk$set(echo = TRUE)
bike <- read.csv("BikeData.csv")
View(bike)
females <- bike[bike$gender == 'F', ]
females[1, 9]
daily<- bike$cyc_freq[0:10] == 'Daily'
daily
sum(daily, na.rm=TRUE)  #
daily<- bike$cyc_freq[0:10] == 'Daily'
sum(daily, na.rm=TRUE)  #
knitr::opts_chunk$set(echo = TRUE)
overtimes <- c(10, 2, 6, 12, 14, 15, 15, 24, 15, 25, 3, 12)
mean(overtimes)
sd(overtimes)
summary(overtimes)
knitr::opts_chunk$set(echo = TRUE)
animaldata <- read.csv("AnimalData.csv")
ncol(animaldata)
nrow(subset(animaldata[1:10,],Outcome.Type=="Adoption"))
subset(animaldata,Intake.Type=="Owner Surrender")[1,]$Neutered.Status=="Neutered"
#Find the number of animals that were adopted
table(animaldata$Outcome.Type)
#Pull out only adopted animals
adopted <- animaldata[animaldata$Outcome.Type=="Adoption",]
#Pull out just the days in shelter for the adopted animals
daystoadopt <- adopted$Days.Shelter
#Visualize and describe this variable
hist(daystoadopt)
fivenum(daystoadopt)
mean(daystoadopt)
sd(daystoadopt)
which(animaldata$Days.Shelter==max(daystoadopt))
(max(daysadopt) - mean(daysadopt)) / sd(daysadopy)
hist(daystoadopt)
(max(daystoadopt) - mean(daystoadopt)) / sd(daystoadopt)
knitr::opts_chunk$set(echo = TRUE)
table(animaldata$Animal.Type[animaldata$Age.Intake>0])
hist(animaldata$Weight[animaldata$Age.Intake>0 & animaldata$Animal.Type =="Dog"])
hist(animaldata$Weight[animaldata$Age.Intake>0 & animaldata$Animal.Type =="Cat"])
1-pnorm((13-mean(catweight))/sd(catweight))
catweight <- animaldata$Weight[animaldata$Age.Intake>0 & animaldata$Animal.Type =="Cat"]
(13-mean(catweight))/sd(catweight)
1-pnorm((13-mean(catweight))/sd(catweight))
knitr::opts_chunk$set(echo = TRUE)
animaldata <- read.csv("AnimalData.csv")
hist(animaldata$Intake.Type)
animaldata <- read.csv("AnimalData.csv")
plot(animaldata$Intake.Type)
knitr::opts_chunk$set(echo = TRUE)
pisaTrain = read.csv("../Data/pisa2009train.csv")
pisaTest = read.csv("../Data/pisa2009test.csv")
nrow(pisaTrain)
tapply(pisaTrain$readingScore, pisaTrain$male, mean)
predTest = predict(lmScore, newdata=pisaTest)
predTest = predict(lmScore, newdata=pisaTest)
install.packages(c("caret", "tidyverse"))
knitr::opts_chunk$set(echo = TRUE)
#############################################################
# Create edx set, validation set, and submission file
#############################################################
# Note: this process could take a couple of minutes
if(!require(tidyverse)) install.packages("tidyverse", repos = "http://cran.us.r-project.org")
if(!require(caret)) install.packages("caret", repos = "http://cran.us.r-project.org")
# MovieLens 10M dataset:
# https://grouplens.org/datasets/movielens/10m/
# http://files.grouplens.org/datasets/movielens/ml-10m.zip
dl <- tempfile()
download.file("http://files.grouplens.org/datasets/movielens/ml-10m.zip", dl)
ratings <- read.table(text = gsub("::", "\t", readLines(unzip(dl, "ml-10M100K/ratings.dat"))),
col.names = c("userId", "movieId", "rating", "timestamp"))
movies <- str_split_fixed(readLines(unzip(dl, "ml-10M100K/movies.dat")), "\\::", 3)
colnames(movies) <- c("movieId", "title", "genres")
movies <- as.data.frame(movies) %>% mutate(movieId = as.numeric(levels(movieId))[movieId],
title = as.character(title),
genres = as.character(genres))
movielens <- left_join(ratings, movies, by = "movieId")
# Validation set will be 10% of MovieLens data
set.seed(1)
test_index <- createDataPartition(y = movielens$rating, times = 1, p = 0.1, list = FALSE)
edx <- movielens[-test_index,]
temp <- movielens[test_index,]
# Make sure userId and movieId in validation set are also in edx set
validation <- temp %>%
semi_join(edx, by = "movieId") %>%
semi_join(edx, by = "userId")
# Add rows removed from validation set back into edx set
removed <- anti_join(temp, validation)
edx <- rbind(edx, removed)
rm(dl, ratings, movies, test_index, temp, movielens, removed)
head(edx)
head(edx)
head(edx)
head(edx)
load("~/GitHub/Harvard University - Data Science Professional/09 - PH125.9x - Capstone/MovieLens Recommender System Project/ml-10M100K/ratings.dat")
knitr::opts_chunk$set(echo = TRUE)
#############################################################
# Create edx set, validation set, and submission file
#############################################################
# Note: this process could take a couple of minutes
if(!require(tidyverse)) install.packages("tidyverse", repos = "http://cran.us.r-project.org")
if(!require(caret)) install.packages("caret", repos = "http://cran.us.r-project.org")
# MovieLens 10M dataset:
# https://grouplens.org/datasets/movielens/10m/
# http://files.grouplens.org/datasets/movielens/ml-10m.zip
dl <- tempfile()
download.file("http://files.grouplens.org/datasets/movielens/ml-10m.zip", dl)
ratings <- read.table(text = gsub("::", "\t", readLines(unzip(dl, "ml-10M100K/ratings.dat"))),
col.names = c("userId", "movieId", "rating", "timestamp"))
setwd("~/GitHub/Harvard University - Data Science Professional/09 - PH125.9x - Capstone/MovieLens Recommender System Project")
movies <- str_split_fixed(readLines(unzip(dl, "ml-10M100K/movies.dat")), "\\::", 3)
colnames(movies) <- c("movieId", "title", "genres")
movies <- as.data.frame(movies) %>% mutate(movieId = as.numeric(levels(movieId))[movieId],
title = as.character(title),
genres = as.character(genres))
movielens <- left_join(ratings, movies, by = "movieId")
# Validation set will be 10% of MovieLens data
set.seed(1)
test_index <- createDataPartition(y = movielens$rating, times = 1, p = 0.1, list = FALSE)
edx <- movielens[-test_index,]
temp <- movielens[test_index,]
# Make sure userId and movieId in validation set are also in edx set
validation <- temp %>%
semi_join(edx, by = "movieId") %>%
semi_join(edx, by = "userId")
# Add rows removed from validation set back into edx set
removed <- anti_join(temp, validation)
edx <- rbind(edx, removed)
rm(dl, ratings, movies, test_index, temp, movielens, removed)
write.csv(edx, "edx.csv")
write.csv(vadlidation, "validation.csv")
# write.csv(edx, "edx.csv")
write.csv(validation, "validation.csv")
str(edx)
# write.csv(edx, "edx.csv")
# write.csv(validation, "validation.csv")
summary(edx)
str(edx)
summary(edx)
head(edx)
head(edx)
knitr::opts_chunk$set(echo = TRUE, fig.align = 'center', cache=TRUE)
# Loading Libraries
library(dplyr)
library(kableExtra)
library(tidyr)
library(stringr)
library(forcats)
# Loading Datasets
edx <-read.csv("edx.csv")
validation <- read.csv("validation.csv")
edx <- edx %>%
mutate(movieGenre = fct_explicit_na(genres,
na_level = "(no genres listed)")
) %>%
separate_rows(movieGenre,
sep = "\\|")
edx$date <- as.POSIXct(edx$timestamp, origin="1970-01-01")
edx$date <- as.POSIXct(edx$timestamp, origin="1970-01-01")
# Extract the year and month of rate
edx$yearOfRate <- format(edx$date,"%Y")
edx$monthOfRate <- format(edx$date,"%m")
# Extract the year of release for each movie
edx <- edx %>%
mutate(title = str_trim(title)) %>%
extract(title,
c("titleTemp", "releaseYear"),
regex = "^(.*) \\(([0-9 \\-]*)\\)$",
remove = F) %>%
mutate(releaseYear = if_else(str_length(releaseYear) > 4,
as.integer(str_split(releaseYear, "-",
simplify = T)[1]),
as.integer(releaseYear))
) %>%
mutate(title = if_else(is.na(titleTemp),
title,
titleTemp)
)
head(edx)
head(edx[:, c(userId)])
head(edx %>% select(userId, movieId))
edx <- edx %>%
mutate(genre = fct_explicit_na(genres,
na_level = "(no genres listed)")
) %>%
separate_rows(genre,
sep = "\\|")
edx <- edx %>% select(userId, movieId, raing, title, genre, release, yearOfRate, monthOfRate)
edx <- edx %>% select(userId, movieId, rating, title, genre, release, yearOfRate, monthOfRate)
# Extract the year of release for each movie
edx <- edx %>%
mutate(title = str_trim(title)) %>%
extract(title,
c("titleTemp", "release"),
regex = "^(.*) \\(([0-9 \\-]*)\\)$",
remove = F) %>%
mutate(releaseYear = if_else(str_length(releaseYear) > 4,
as.integer(str_split(releaseYear, "-",
simplify = T)[1]),
as.integer(releaseYear))
) %>%
mutate(title = if_else(is.na(titleTemp),
title,
titleTemp)
) %>%
select(-titleTemp)
